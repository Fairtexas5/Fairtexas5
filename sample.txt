import pandas as pd
import numpy as np
from datetime import datetime
import logging
import warnings

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Configuration dictionary for reusability
CONFIG = {
    'transaction_files': {
        'JAN': 'jan_transactions.csv',
        'FEB': 'feb_transactions.csv',
        'MAR': 'mar_transactions.csv'
    },
    'month_prefixes': ['JAN25', 'FEB25', 'MAR25'],
    'key_txn_cols': {
        'UPI_CR_CNT': 'sum', 'UPI_DR_CNT': 'sum', 'UPI_CR_AMT': 'sum', 'UPI_DR_AMT': 'sum',
        'NEFT_CR_CNT': 'sum', 'NEFT_DR_CNT': 'sum', 'NEFT_CR_AMT': 'sum', 'NEFT_DR_AMT': 'sum',
        'RTGS_CR_CNT': 'sum', 'RTGS_DR_CNT': 'sum', 'RTGS_CR_AMT': 'sum', 'RTGS_DR_AMT': 'sum',
        'INB_CR_CNT': 'sum', 'INB_DR_CNT': 'sum', 'INB_CR_AMT': 'sum', 'INB_DR_AMT': 'sum'
    },
    'main_file': 'Book1_small.csv',
    'customer_file': 'customer_info.csv',
    'date_columns': ['ACCT_OPN_DT', 'REPORT_DT', 'LST_CUST_CR_DT', 'LST_CUST_DR_DT'],
    'observation_period_days': 90,
    'base_weights': {
        'digital': {'INB': 5, 'YONO': 4, 'MBS': 3, 'UPI': 2, 'ATM': 1},
        'transaction': {'DR_AMT': 1, 'CR_AMT': 1},
        'product': {'FD': 3, 'RD': 2, 'PERSONAL_LOAN': 4, 'HOME_LOAN': 5}
    },
    'feature_categories': {
        'Identity': ['CUST_NBR', 'CUST_CLASS', 'CUST_CAT', 'CUST_CRTN', 'HOME_BRCH_NBR', 'CUST_AGE', 'MRTL_STS'],
        'Account_Info': ['JNT_ACCT_FLAG', 'JNT_ACCT_FLG_NUM', 'ACCT_OPN_DT', 'TENURE_DAYS'],
        'Balances': ['END_OF_DAY_BAL', 'AVG_BAL_MTD', 'AVG_BAL_QTD', 'AVG_BAL_YTD', 'AQB', 'TDV'],
        'Transactions': ['DR_AMT', 'CR_AMT', 'DR_NO', 'CR_NO', 'TXN_FREQ', 'AVG_TXN_AMT', 'AVG_DR_AMT', 'AVG_CR_AMT'],
        'Products': ['TOTAL_PRODUCT', 'TOTAL_PRODUCT_EXCL_SERVICES', 'SAVINGS_BANK', 'FD', 'RD', 'PERSONAL_LOAN', 'HOME_LOAN'],
        'Digital_Channels': ['ATM', 'MBS', 'YONO', 'UPI', 'INB', 'DIGITAL_ENGAGE', 'CHNNL_SOPHISTICATION'],
        'Behavioral': ['TXN_VELOCITY', 'TXN_INTENSITY', 'BAL_UTIL', 'CROSS_SELL_SUCCESS', 'RELATION_DEPTH'],
        'Risk_Indicators': ['DORM_RISK', 'SEV_DORM_RISK', 'DAY_SINCE_LST_TXN', 'DAYS_SINCE_LAST_CR', 'DAYS_SINCE_LAST_DR'],
        'Ratios': ['NET_FLOW_RATIO', 'CR_TO_DR_RATIO', 'TXN_CNT_RATIO', 'PROD_DIVERSITY', 'LOAN_TO_INVEST', 'DIGI_TO_PHYS'],
        'Scores': ['transaction_score', 'product_score', 'INVEST_ORIENT', 'LOAN_SOPHISTICATION', 'SERV_ADOPT'],
        'Segments': ['ACT_LVL', 'PREM_BANK']
    }
}

# --- Step 1: Load and Process Transaction Data ---
logger.info("Step 1: Loading and processing monthly transaction data...")

def process_transaction_data(config):
    monthly_txn_data = []
    month_names = list(config['transaction_files'].keys())
    
    for month, file_path in config['transaction_files'].items():
        try:
            df = pd.read_csv(file_path)
            logger.info(f"Loaded {month} transactions: {len(df)} records")
            
            # Standardize column names
            prefix = config['month_prefixes'][month_names.index(month)]
            df.columns = [col.replace(f'_{prefix}', '') for col in df.columns]
            
            # Validate required columns
            existing_cols = [col for col in config['key_txn_cols'].keys() if col in df.columns]
            if not existing_cols:
                logger.warning(f"No valid transaction columns found for {month}")
                continue
                
            # Aggregate by account
            agg_dict = {col: config['key_txn_cols'][col] for col in existing_cols}
            month_agg = df.groupby('ACCT_NBR').agg(agg_dict).reset_index()
            
            # Compute totals and ratios
            dr_cnt_cols = [col for col in existing_cols if 'DR_CNT' in col]
            cr_cnt_cols = [col for col in existing_cols if 'CR_CNT' in col]
            dr_amt_cols = [col for col in existing_cols if 'DR_AMT' in col]
            cr_amt_cols = [col for col in existing_cols if 'CR_AMT' in col]
            
            month_agg[f'{month}_DR_CNT'] = month_agg[dr_cnt_cols].sum(axis=1) if dr_cnt_cols else 0
            month_agg[f'{month}_CR_CNT'] = month_agg[cr_cnt_cols].sum(axis=1) if cr_cnt_cols else 0
            month_agg[f'{month}_DR_AMT'] = month_agg[dr_amt_cols].sum(axis=1) if dr_amt_cols else 0
            month_agg[f'{month}_CR_AMT'] = month_agg[cr_amt_cols].sum(axis=1) if cr_amt_cols else 0
            
            # High-value and digital indicators
            month_agg[f'{month}_HV_AMT'] = month_agg.get('RTGS_CR_AMT', 0) + month_agg.get('RTGS_DR_AMT', 0)
            month_agg[f'{month}_HV_CNT'] = month_agg.get('RTGS_CR_CNT', 0) + month_agg.get('RTGS_DR_CNT', 0)
            
            digital_amt_cols = [col for col in existing_cols if any(prefix in col for prefix in ['UPI_', 'INB_']) and 'AMT' in col]
            digital_cnt_cols = [col for col in existing_cols if any(prefix in col for prefix in ['UPI_', 'INB_']) and 'CNT' in col]
            month_agg[f'{month}_DIG_AMT'] = month_agg[digital_amt_cols].sum(axis=1) if digital_amt_cols else 0
            month_agg[f'{month}_DIG_CNT'] = month_agg[digital_cnt_cols].sum(axis=1) if digital_cnt_cols else 0
            
            # Keep key columns
            key_cols = ['ACCT_NBR', f'{month}_DR_CNT', f'{month}_CR_CNT', f'{month}_DR_AMT', f'{month}_CR_AMT',
                       f'{month}_HV_AMT', f'{month}_HV_CNT', f'{month}_DIG_AMT', f'{month}_DIG_CNT']
            month_agg = month_agg[key_cols]
            monthly_txn_data.append(month_agg)
            
        except Exception as e:
            logger.warning(f"Failed to process {month} transactions: {e}")
            continue
    
    if not monthly_txn_data:
        logger.error("No transaction data processed")
        return None
        
    # Merge monthly data
    txn_account = monthly_txn_data[0]
    for month_data in monthly_txn_data[1:]:
        txn_account = txn_account.merge(month_data, on='ACCT_NBR', how='outer')
    
    txn_account = txn_account.fillna(0)
    
    # Compute quarterly aggregates
    for metric in ['DR_CNT', 'CR_CNT', 'DR_AMT', 'CR_AMT', 'HV_AMT', 'HV_CNT', 'DIG_AMT', 'DIG_CNT']:
        cols = [f'{month}_{metric}' for month in month_names]
        txn_account[f'Q1_{metric}'] = txn_account[cols].sum(axis=1)
    
    # Compute ratios and trends
    total_amt = txn_account['Q1_DR_AMT'] + txn_account['Q1_CR_AMT']
    total_cnt = txn_account['Q1_DR_CNT'] + txn_account['Q1_CR_CNT']
    
    txn_account['HV_AMT_RATIO'] = txn_account['Q1_HV_AMT'] / (total_amt + 1e-6)
    txn_account['DIG_AMT_RATIO'] = txn_account['Q1_DIG_AMT'] / (total_amt + 1e-6)
    txn_account['AVG_TXN_VALUE'] = total_amt / (total_cnt + 1e-6)
    
    txn_account['TXN_CNT_GROWTH'] = ((txn_account['MAR_DR_CNT'] + txn_account['MAR_CR_CNT']) -
                                    (txn_account['JAN_DR_CNT'] + txn_account['JAN_CR_CNT'])) / \
                                   ((txn_account['JAN_DR_CNT'] + txn_account['JAN_CR_CNT']) + 1e-6)
    
    txn_account['TXN_AMT_GROWTH'] = ((txn_account['MAR_DR_AMT'] + txn_account['MAR_CR_AMT']) -
                                    (txn_account['JAN_DR_AMT'] + txn_account['JAN_CR_AMT'])) / \
                                   ((txn_account['JAN_DR_AMT'] + txn_account['JAN_CR_AMT']) + 1e-6)
    
    # Transaction consistency
    monthly_totals = pd.DataFrame({
        f'{month}_TOTAL': txn_account[f'{month}_DR_AMT'] + txn_account[f'{month}_CR_AMT']
        for month in month_names
    })
    txn_account['TXN_CONSISTENCY'] = 1 / (monthly_totals.std(axis=1) / (monthly_totals.mean(axis=1) + 1e-6) + 1e-6)
    
    # Final feature selection
    final_cols = ['ACCT_NBR', 'Q1_DR_CNT', 'Q1_CR_CNT', 'Q1_DR_AMT', 'Q1_CR_AMT',
                 'Q1_HV_AMT', 'Q1_HV_CNT', 'Q1_DIG_AMT', 'Q1_DIG_CNT',
                 'HV_AMT_RATIO', 'DIG_AMT_RATIO', 'AVG_TXN_VALUE',
                 'TXN_CNT_GROWTH', 'TXN_AMT_GROWTH', 'TXN_CONSISTENCY']
    
    logger.info(f"Transaction data processed: {len(txn_account)} accounts with {len(final_cols)-1} features")
    return txn_account[final_cols]

txn_account = process_transaction_data(CONFIG)

# --- Step 2: Load Main Dataset and Merge ---
logger.info("Step 2: Loading main dataset and merging with transactions...")

try:
    df = pd.read_csv(CONFIG['main_file'])
    logger.info(f"Main dataset loaded: {len(df)} records")
    
    # Convert date columns
    for col in CONFIG['date_columns']:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], format='%d/%m/%y', errors='coerce')
    
    reference_date = df['REPORT_DT'].max() if 'REPORT_DT' in df.columns else pd.Timestamp.now()
    
    # Filter for savings bank accounts
    df_sb = df[df['PF_FAC_REPT'] == 'SB'].copy() if 'PF_FAC_REPT' in df.columns else df.copy()
    
    # Merge with transaction data
    if txn_account is not None:
        logger.info(f"Merging {len(df_sb)} accounts with {len(txn_account)} transaction records...")
        df_sb = df_sb.merge(txn_account, on='ACCT_NBR', how='left')
        df_sb.update(df_sb[['Q1_DR_CNT', 'Q1_CR_CNT', 'Q1_DR_AMT', 'Q1_CR_AMT']].fillna(0))
        df_sb['DR_NO'] = df_sb.get('Q1_DR_CNT', df_sb.get('DR_NO', 0))
        df_sb['CR_NO'] = df_sb.get('Q1_CR_CNT', df_sb.get('CR_NO', 0))
        df_sb['DR_AMT'] = df_sb.get('Q1_DR_AMT', df_sb.get('DR_AMT', 0))
        df_sb['CR_AMT'] = df_sb.get('Q1_CR_AMT', df_sb.get('CR_AMT', 0))
        logger.info("Transaction merge completed")
    else:
        logger.warning("No transaction data to merge")
    
    logger.info(f"Main dataset ready: {len(df_sb)} records")
except Exception as e:
    logger.error(f"Failed to load or merge main dataset: {e}")
    raise

# --- Step 3: Load Customer Info ---
logger.info("Step 3: Loading customer info...")

def load_customer_info(file_path):
    try:
        customer_info = pd.read_csv(file_path)
        if 'CUST_AGE' in customer_info.columns:
            customer_info['CUST_AGE'] = pd.to_numeric(customer_info['CUST_AGE'], errors='coerce').clip(upper=100)
        if 'MRTL_STS' in customer_info.columns:
            customer_info['MRTL_STS'] = customer_info['MRTL_STS'].fillna('U')
        if 'PANFLAG' in customer_info.columns:
            customer_info['PANFLAG'] = customer_info['PANFLAG'].apply(lambda x: 1 if pd.notna(x) and x == 8.0 else 0)
        if 'MOBILE_NBR' in customer_info.columns:
            customer_info['MOBILE_NBR'] = customer_info['MOBILE_NBR'].apply(lambda x: 1 if pd.notna(x) and x == 8.0 else 0)
        logger.info(f"Customer info loaded: {len(customer_info)} customers")
        return customer_info
    except Exception as e:
        logger.warning(f"Failed to load customer info: {e}")
        return None

customer_info = load_customer_info(CONFIG['customer_file'])

# --- Step 4: Account-Level Feature Engineering ---
logger.info("Step 4: Computing engagement scores...")

def compute_weights(df, features, base_weights, group_col='CUST_CLASS'):
    if not features or group_col not in df.columns:
        return {}
    
    weights = {}
    overall_usage = df[features].mean()
    tier_usage = df.groupby(group_col)[features].mean()
    
    for tier in tier_usage.index:
        weights[tier] = {}
        for feature in features:
            base_weight = base_weights.get(feature, 1)
            tier_rate = tier_usage.loc[tier, feature]
            overall_rate = overall_usage[feature]
            weights[tier][feature] = base_weight * (tier_rate / (overall_rate + 0.01))
    
    return weights

# Define feature groups
digital_features = [f for f in ['ATM', 'MBS', 'YONO', 'UPI', 'INB'] if f in df_sb.columns]
transaction_features = [f for f in ['DR_AMT', 'CR_AMT'] if f in df_sb.columns]
product_features = [f for f in ['FD', 'RD', 'PERSONAL_LOAN', 'HOME_LOAN'] if f in df_sb.columns]

# Compute weights
weights_digital = compute_weights(df_sb, digital_features, CONFIG['base_weights']['digital'])
weights_transaction = compute_weights(df_sb, transaction_features, CONFIG['base_weights']['transaction'])
weights_product = compute_weights(df_sb, product_features, CONFIG['base_weights']['product'])

# Compute scores
def compute_score(row, features, weights, default_weight=1):
    tier = row.get('CUST_CLASS', 'DEFAULT')
    if tier not in weights:
        return sum(row.get(f, 0) for f in features)
    return sum(weights[tier].get(f, default_weight) * row.get(f, 0) for f in features)

df_sb['DIGITAL_ENGAGE'] = df_sb.apply(lambda row: compute_score(row, digital_features, weights_digital), axis=1)
df_sb['transaction_score'] = df_sb.apply(lambda row: compute_score(row, transaction_features, weights_transaction), axis=1)
df_sb['product_score'] = df_sb.apply(lambda row: compute_score(row, product_features, weights_product), axis=1)

# --- Step 5: Customer-Level Aggregation ---
logger.info("Step 5: Aggregating to customer level...")

def build_agg_dict(df):
    agg_dict = {
        'CUST_CLASS': 'first',
        'CUST_CAT': lambda x: x.mode().iloc[0] if not x.mode().empty else 'Unknown'
    }
    
    for field in ['SALARY', 'STAFF', 'NRI', 'AGRI', 'SME', 'SBF', 'SSI']:
        if field in df.columns:
            agg_dict[field] = 'max'
    
    for field in ['END_OF_DAY_BAL', 'AVG_BAL_MTD', 'AVG_BAL_QTD', 'AVG_BAL_YTD', 'AQB', 'TDV',
                  'DR_AMT', 'CR_AMT', 'DR_NO', 'CR_NO', 'HOME_LOAN_AMT', 'DIGITAL_ENGAGE',
                  'transaction_score', 'product_score']:
        if field in df.columns:
            agg_dict[field] = 'sum'
    
    for field in ['SAVINGS_BANK', 'FD', 'RD', 'PERSONAL_LOAN', 'HOME_LOAN', 'CAR_LOAN',
                  'EDUCATION_LOAN', 'GOLD_LOAN', 'AGRI_LOAN', 'SBI_MF', 'SBI_CAP', 'SBI_CARD',
                  'SBI_LFE', 'SBI_GNRL', 'APY', 'NPS', 'PMJJY', 'PMSBY', 'ATM', 'MBS', 'YONO',
                  'UPI', 'INB', 'LOCKER']:
        if field in df.columns:
            agg_dict[field] = 'max'
    
    if 'JNT_ACCT_FLAG' in df.columns:
        agg_dict['JNT_ACCT_FLAG'] = lambda x: x.mode().iloc[0] if not x.mode().empty else 'N'
    
    for field, agg_func in {'ACCT_OPN_DT': 'min', 'REPORT_DT': 'max',
                           'LST_CUST_CR_DT': 'max', 'LST_CUST_DR_DT': 'max'}.items():
        if field in df.columns:
            agg_dict[field] = agg_func
    
    if 'TENURE_DAYS' in df.columns:
        agg_dict['TENURE_DAYS'] = 'max'
    
    return agg_dict

customer_agg_dict = build_agg_dict(df_sb)
df_customer = df_sb.groupby('CUST_NBR').agg(customer_agg_dict).reset_index()
logger.info(f"Customer aggregation complete: {len(df_customer)} customers")

# --- Step 6: Merge with Customer Info ---
logger.info("Step 6: Merging with customer info...")

if customer_info is not None:
    merge_cols = ['CUST_NBR'] + [col for col in ['CUST_CRTN', 'HOME_BRCH_NBR', 'PANFLAG', 'CUST_AGE', 'MRTL_STS', 'MOBILE_NBR'] if col in customer_info.columns]
    df_customer = df_customer.merge(customer_info[merge_cols], on='CUST_NBR', how='left')
    logger.info("Customer info merged successfully")

# --- Step 7: Post-Aggregation Feature Engineering ---
logger.info("Step 7: Creating derived features...")

# Define service columns
service_cols = [col for col in ['SBI_CARD', 'SBI_LFE', 'SBI_GNRL', 'APY', 'PMJJY', 'PMSBY'] if col in df_customer.columns]

# Basic counts
df_customer['TXN_FREQ'] = df_customer.get('DR_NO', 0) + df_customer.get('CR_NO', 0)
df_customer['JNT_ACCT_FLG_NUM'] = df_customer.get('JNT_ACCT_FLAG', 'N').map({'Y': 2, 'N': 1}).fillna(1)

# Product counts
product_cols = [col for col in CONFIG['feature_categories']['Products'] if col in df_customer.columns and col not in ['TOTAL_PRODUCT', 'TOTAL_PRODUCT_EXCL_SERVICES']]
if product_cols:
    df_customer['TOTAL_PRODUCT'] = df_customer[product_cols].eq(1).sum(axis=1)
    df_customer['TOTAL_PRODUCT_EXCL_SERVICES'] = df_customer['TOTAL_PRODUCT'] - df_customer[service_cols].eq(1).sum(axis=1) if service_cols else df_customer['TOTAL_PRODUCT']
else:
    df_customer['TOTAL_PRODUCT'] = 0
    df_customer['TOTAL_PRODUCT_EXCL_SERVICES'] = 0

# Behavioral features
df_customer['TXN_VELOCITY'] = df_customer['TXN_FREQ'] / CONFIG['observation_period_days']
df_customer['MTH_TXN_RATE'] = df_customer['TXN_FREQ'] / 3
df_customer['AVG_TXN_AMT'] = (df_customer.get('DR_AMT', 0) + df_customer.get('CR_AMT', 0)) / (df_customer['TXN_FREQ'] + 1e-6)

# Balance ratios
if 'AVG_BAL_QTD' in df_customer.columns and 'END_OF_DAY_BAL' in df_customer.columns:
    df_customer['BAL_STAB'] = df_customer['AVG_BAL_QTD'] / (df_customer['END_OF_DAY_BAL'] + 1e-6)
    df_customer['BAL_GTH_IND'] = df_customer['END_OF_DAY_BAL'] / (df_customer['AVG_BAL_QTD'] + 1e-6)
if 'AVG_BAL_MTD' in df_customer.columns and 'AVG_BAL_QTD' in df_customer.columns:
    df_customer['MTH_BAL_TREND'] = df_customer['AVG_BAL_MTD'] / (df_customer['AVG_BAL_QTD'] + 1e-6)

# Investment, loan, and service features
investment_products = [col for col in ['FD', 'RD', 'SBI_MF', 'SBI_CAP', 'NPS'] if col in df_customer.columns]
df_customer['INVEST_ORIENT'] = df_customer[investment_products].sum(axis=1) if investment_products else 0
df_customer['INVEST_DIVERSITY'] = df_customer[investment_products].gt(0).sum(axis=1) if investment_products else 0

loan_products = [col for col in ['PERSONAL_LOAN', 'HOME_LOAN', 'CAR_LOAN', 'EDUCATION_LOAN'] if col in df_customer.columns]
df_customer['LOAN_SOPHISTICATION'] = df_customer[loan_products].sum(axis=1) if loan_products else 0
secured_loans = [col for col in ['HOME_LOAN', 'CAR_LOAN'] if col in df_customer.columns]
df_customer['SEC_LOAN_PREF'] = df_customer[secured_loans].sum(axis=1) / (df_customer['LOAN_SOPHISTICATION'] + 1e-6) if secured_loans else 0

df_customer['SERV_ADOPT'] = df_customer[service_cols].sum(axis=1) if service_cols else 0

# Premium banking indicator
if 'CUST_CLASS' in df_customer.columns and 'AVG_BAL_QTD' in df_customer.columns:
    premium_classes = ['RHODIUM', 'PLATINUM', 'DIAMOND']
    bal_threshold = df_customer['AVG_BAL_QTD'].quantile(0.8)
    df_customer['PREM_BANK'] = ((df_customer['CUST_CLASS'].isin(premium_classes)) & (df_customer['AVG_BAL_QTD'] > bal_threshold)).astype(int)
else:
    df_customer['PREM_BANK'] = 0

# Channel sophistication
channel_weights = {'YONO': 3, 'INB': 2, 'UPI': 1.5, 'MBS': 1, 'ATM': 0.5}
df_customer['CHNNL_SOPHISTICATION'] = sum(df_customer.get(channel, 0) * weight for channel, weight in channel_weights.items() if channel in df_customer.columns)

# Cross-sell and relationship depth
df_customer['CROSS_SELL_SUCCESS'] = df_customer['TOTAL_PRODUCT'] / ((df_customer.get('TENURE_DAYS', 0) / 365) + 1e-6)
df_customer['RELATION_DEPTH'] = (df_customer['INVEST_ORIENT'] + df_customer['LOAN_SOPHISTICATION'] +
                                df_customer['SERV_ADOPT'] + df_customer.get('DIGITAL_ENGAGE', 0))

# Additional features
df_customer['AVG_DR_AMT'] = df_customer.get('DR_AMT', 0) / (df_customer.get('DR_NO', 0) + 1e-6)
df_customer['AVG_CR_AMT'] = df_customer.get('CR_AMT', 0) / (df_customer.get('CR_NO', 0) + 1e-6)

if 'LST_CUST_CR_DT' in df_customer.columns:
    df_customer['DAYS_SINCE_LAST_CR'] = (reference_date - df_customer['LST_CUST_CR_DT']).dt.days
if 'LST_CUST_DR_DT' in df_customer.columns:
    df_customer['DAYS_SINCE_LAST_DR'] = (reference_date - df_customer['LST_CUST_DR_DT']).dt.days

existing_date_cols = [col for col in ['DAYS_SINCE_LAST_CR', 'DAYS_SINCE_LAST_DR'] if col in df_customer.columns]
df_customer['DAY_SINCE_LST_TXN'] = df_customer[existing_date_cols].min(axis=1, skipna=True) if existing_date_cols else np.nan

df_customer['NET_FLOW_AMT'] = df_customer.get('CR_AMT', 0) - df_customer.get('DR_AMT', 0)
total_flow = df_customer.get('CR_AMT', 0) + df_customer.get('DR_AMT', 0)
df_customer['NET_FLOW_RATIO'] = df_customer['NET_FLOW_AMT'] / (total_flow + 1e-6)
df_customer['BAL_UTIL'] = total_flow / (df_customer.get('AVG_BAL_QTD', 0) + 1e-6)
df_customer['MTH_BAL_UTIL'] = df_customer['BAL_UTIL'] / 3

core_products = [col for col in ['SAVINGS_BANK', 'FD', 'RD', 'PERSONAL_LOAN', 'HOME_LOAN', 'SBI_MF', 'UPI', 'INB'] if col in df_customer.columns]
df_customer['PROD_DIVERSITY'] = df_customer[core_products].gt(0).sum(axis=1) / len(core_products) if core_products else 0

loan_cols = [col for col in ['PERSONAL_LOAN', 'HOME_LOAN'] if col in df_customer.columns]
invest_cols = [col for col in ['FD', 'RD', 'SBI_MF'] if col in df_customer.columns]
df_customer['LOAN_TO_INVEST'] = df_customer[loan_cols].sum(axis=1) / (df_customer[invest_cols].sum(axis=1) + 1e-6) if loan_cols and invest_cols else 0

digital_cols = [col for col in ['MBS', 'YONO', 'UPI', 'INB'] if col in df_customer.columns]
physical_cols = [col for col in ['ATM', 'LOCKER'] if col in df_customer.columns]
df_customer['DIGI_TO_PHYS'] = df_customer[digital_cols].sum(axis=1) / (df_customer[physical_cols].sum(axis=1) + 1e-6) if digital_cols and physical_cols else 0

df_customer['JNT_ACCT_DR_AMT'] = df_customer.get('DR_AMT', 0) / df_customer['JNT_ACCT_FLG_NUM']
df_customer['DORM_RISK'] = (df_customer['DAY_SINCE_LST_TXN'] > 30).astype(int)
df_customer['SEV_DORM_RISK'] = (df_customer['DAY_SINCE_LST_TXN'] > 60).astype(int)
df_customer['TXN_INTENSITY'] = df_customer['TXN_FREQ'] / CONFIG['observation_period_days']
df_customer['VAL_INTENSITY'] = total_flow / CONFIG['observation_period_days']
df_customer['TENURE_TXN_INTERACT'] = df_customer.get('TENURE_DAYS', 0) * df_customer['TXN_INTENSITY']
df_customer['PROD_BAL_INTERACT'] = df_customer['TOTAL_PRODUCT'] * df_customer.get('AVG_BAL_QTD', 0)
df_customer['CR_TO_DR_RATIO'] = df_customer.get('CR_AMT', 0) / (df_customer.get('DR_AMT', 0) + 1e-6)
df_customer['TXN_CNT_RATIO'] = df_customer.get('CR_NO', 0) / (df_customer.get('DR_NO', 0) + 1e-6)
df_customer['ACT_LVL'] = pd.cut(df_customer['TXN_FREQ'], bins=[-np.inf, 5, 20, 50, np.inf], labels=['Low', 'Medium', 'High', 'Very_High'])

# --- Step 8: Handle Missing Values ---
logger.info("Step 8: Handling missing values...")

for col in df_customer.select_dtypes(include=[np.number]).columns:
    if col in ['TENURE_DAYS', 'CUST_CRTN', 'CUST_AGE']:
        df_customer[col] = df_customer[col].fillna(df_customer[col].median())
    elif col in ['CROSS_SELL_SUCCESS']:
        df_customer[col] = df_customer[col].fillna(0)
    elif 'DAYS_SINCE_LAST' in col:
        df_customer[col] = df_customer[col].fillna(df_customer[col].max())
    else:
        df_customer[col] = df_customer[col].fillna(0)

if 'HOME_BRCH_NBR' in df_customer.columns:
    df_customer['HOME_BRCH_NBR'] = df_customer['HOME_BRCH_NBR'].fillna('Unknown')
if 'MRTL_STS' in df_customer.columns:
    df_customer['MRTL_STS'] = df_customer['MRTL_STS'].fillna('U')

# Recalculate DAY_SINCE_LST_TXN
existing_date_cols = [col for col in ['DAYS_SINCE_LAST_CR', 'DAYS_SINCE_LAST_DR'] if col in df_customer.columns]
if existing_date_cols:
    df_customer['DAY_SINCE_LST_TXN'] = df_customer[existing_date_cols].min(axis=1, skipna=True)

# --- Step 9: Data Quality Checks ---
logger.info("Step 9: Performing data quality checks...")

for col in df_customer.select_dtypes(include=[np.number]).columns:
    if np.isinf(df_customer[col]).any():
        df_customer[col] = df_customer[col].replace([np.inf, -np.inf], np.nan).fillna(df_customer[col].median())
        logger.warning(f"Fixed infinite values in {col}")

for col in df_customer.select_dtypes(include=[np.number]).columns:
    q99 = df_customer[col].quantile(0.99)
    if q99 > 0 and not pd.isna(q99):
        outlier_count = (df_customer[col] > q99 * 10).sum()
        if outlier_count > 0:
            df_customer[col] = df_customer[col].clip(upper=q99 * 10)
            logger.info(f"Capped {outlier_count} extreme values in {col}")

# --- Step 10: Feature Selection and Final Preparation ---
logger.info("Step 10: Preparing final dataset...")

total_features = sum(len([f for f in features if f in df_customer.columns]) for features in CONFIG['feature_categories'].values())
logger.info(f"\nFeature Summary: {total_features} features across {len(CONFIG['feature_categories'])} categories")
for category, features in CONFIG['feature_categories'].items():
    existing_features = [f for f in features if f in df_customer.columns]
    logger.info(f"{category}: {len(existing_features)} features")

duplicates = df_customer.duplicated(subset=['CUST_NBR']).sum()
if duplicates > 0:
    logger.warning(f"Removed {duplicates} duplicate customers")
    df_customer = df_customer.drop_duplicates(subset=['CUST_NBR'])

# --- Step 11: Save Output ---
logger.info("Step 11: Saving final dataset...")

try:
    df_customer.to_csv('customer_segmentation_data.csv', index=False)
    feature_mapping = pd.DataFrame([
        {'Feature': feature, 'Category': category}
        for category, features in CONFIG['feature_categories'].items()
        for feature in features if feature in df_customer.columns
    ])
    feature_mapping.to_csv('feature_mapping.csv', index=False)
    summary_stats = df_customer.describe()
    summary_stats.to_csv('data_summary.csv')
    logger.info("Output files saved successfully")
except Exception as e:
    logger.error(f"Error saving files: {e}")

# --- Step 12: Final Validation ---
logger.info("Step 12: Final validation...")

logger.info("\nData types summary:")
logger.info(df_customer.dtypes.value_counts())

key_columns = [col for col in ['CUST_NBR', 'TXN_FREQ', 'TOTAL_PRODUCT', 'DIGITAL_ENGAGE'] if col in df_customer.columns]
missing_summary = df_customer[key_columns].isnull().sum()
logger.info(f"\nMissing values in key columns:\n{missing_summary}")

logger.info(f"\nBasic statistics:")
logger.info(f"Average transaction frequency: {df_customer['TXN_FREQ'].mean():.2f}")
logger.info(f"Average products per customer: {df_customer['TOTAL_PRODUCT'].mean():.2f}")
logger.info(f"Average digital engagement: {df_customer.get('DIGITAL_ENGAGE', pd.Series([0])).mean():.2f}")

if 'ACT_LVL' in df_customer.columns:
    logger.info(f"\nActivity level distribution:\n{df_customer['ACT_LVL'].value_counts()}")

logger.info("\n" + "="*50)
logger.info("CUSTOMER SEGMENTATION DATA PROCESSING COMPLETE")
logger.info(f"Final dataset shape: {df_customer.shape}")
logger.info("Output file: customer_segmentation_data.csv")
logger.info("="*50)
