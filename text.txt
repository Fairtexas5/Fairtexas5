import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler
from sklearn.feature_selection import VarianceThreshold
from sklearn.decomposition import IncrementalPCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming df_customer is your DataFrame loaded with the provided dataset
# Replace with your actual data loading step if needed
# df_customer = pd.read_csv('your_data.csv')

# Step 1: Drop Unnecessary Features
columns_to_drop = ['CUST_NBR', 'ACCT_OPN_DT', 'REPORT_DT', 'LST_CUST_CR_DT', 'LST_CUST_DR_DT']
df_customer = df_customer.drop(columns=[col for col in columns_to_drop if col in df_customer.columns], errors='ignore')

# Drop low-variance features
numeric_cols = df_customer.select_dtypes(include=[np.number]).columns
selector = VarianceThreshold(threshold=0.01)
selector.fit(df_customer[numeric_cols])
low_variance_cols = numeric_cols[~selector.get_support()]
df_customer = df_customer.drop(columns=low_variance_cols, errors='ignore')
print(f"Dropped low-variance columns: {low_variance_cols.tolist()}")

# Step 2: Encode Categorical Features
nominal_features = [col for col in ['CUST_CLASS', 'CUST_CAT', 'JNT_ACCT_FLG', 'HIGHEST_TIER', 'MRTL_STS'] if col in df_customer.columns]
onehot = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
onehot_encoded = onehot.fit_transform(df_customer[nominal_features])
onehot_df = pd.DataFrame(onehot_encoded, columns=onehot.get_feature_names_out(nominal_features), index=df_customer.index)

ordinal_feature = ['ACT_LVL']
if 'ACT_LVL' in df_customer.columns:
    ordinal = OrdinalEncoder(categories=[['Low', 'Medium', 'High', 'Very_High']])
    ordinal_encoded = ordinal.fit_transform(df_customer[ordinal_feature])
    ordinal_df = pd.DataFrame(ordinal_encoded, columns=['ACT_LVL_ordinal'], index=df_customer.index)
else:
    ordinal_df = pd.DataFrame(0, columns=['ACT_LVL_ordinal'], index=df_customer.index)
    print("Warning: 'ACT_LVL' not found, setting to 0.")

df_customer = df_customer.drop(columns=nominal_features + ordinal_feature, errors='ignore')
df_customer = pd.concat([df_customer, onehot_df, ordinal_df], axis=1)

# Step 3: Normalize Features
numeric_features = df_customer.select_dtypes(include=[np.number]).columns
minmax_features = [col for col in ['HV_AMT_RATIO', 'DIG_AMT_RATIO', 'NET_FLOW_RATIO', 'CR_TO_DR_RATIO', 
                                  'TXN_CNT_RATIO', 'PROD_DIVERSITY', 'LOAN_TO_INVEST', 'DIGI_TO_PHYS', 
                                  'SEC_LOAN_PREF'] if col in df_customer.columns]
standard_features = [col for col in numeric_features if col not in minmax_features]

scaler_standard = StandardScaler()
scaled_standard = scaler_standard.fit_transform(df_customer[standard_features])
scaled_standard_df = pd.DataFrame(scaled_standard, columns=standard_features, index=df_customer.index)

scaler_minmax = MinMaxScaler()
scaled_minmax = scaler_minmax.fit_transform(df_customer[minmax_features])
scaled_minmax_df = pd.DataFrame(scaled_minmax, columns=minmax_features, index=df_customer.index)

df_customer[standard_features] = scaled_standard_df
df_customer[minmax_features] = scaled_minmax_df

# Step 4: Dimensionality Reduction with Incremental PCA
ipca = IncrementalPCA(n_components=10, batch_size=1000)  # Batch size for memory efficiency
X_ipca = ipca.fit_transform(df_customer[numeric_features])
pca_df = pd.DataFrame(X_ipca, columns=[f'PC{i+1}' for i in range(10)], index=df_customer.index)
print(f"Explained variance ratio: {sum(ipca.explained_variance_ratio_):.4f}")

# Step 5: Clustering with K-means
kmeans = KMeans(n_clusters=5, random_state=42, n_init=10, n_jobs=-1)  # Use all 5 vCPUs
clusters = kmeans.fit_predict(pca_df)
df_customer['Cluster'] = clusters

# Evaluate clustering
sil_score = silhouette_score(pca_df, clusters, sample_size=10000, random_state=42)
print(f"Silhouette Score: {sil_score}")

# Step 6: Visualize and Interpret Clusters
plt.figure(figsize=(10, 6))
sns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], hue=df_customer['Cluster'], palette='tab10')
plt.title('Customer Segments in PCA Space')
plt.savefig('cluster_plot.png')
plt.close()

# Summarize clusters (focus on key features for high-value segmentation)
key_features = ['AVG_BAL_QTD', 'TXN_FREQ', 'DIGITAL_ENGAGE', 'TOTAL_PRODUCT', 'PREM_BANK', 'ACT_LVL_ordinal']
cluster_summary = df_customer.groupby('Cluster')[key_features].mean()
print("Cluster Summary (Key Features):")
print(cluster_summary)

# Save results
df_customer.to_csv('segmented_customers.csv', index=False)
print("Results saved to 'segmented_customers.csv'")
